<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://jtbradt.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://jtbradt.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-01-15T15:52:49+00:00</updated><id>https://jtbradt.github.io/feed.xml</id><title type="html">blank</title><subtitle></subtitle><entry><title type="html">A Policy by Any Other Name</title><link href="https://jtbradt.github.io/blog/2023/jmp/" rel="alternate" type="text/html" title="A Policy by Any Other Name"/><published>2023-10-08T00:00:00+00:00</published><updated>2023-10-08T00:00:00+00:00</updated><id>https://jtbradt.github.io/blog/2023/jmp</id><content type="html" xml:base="https://jtbradt.github.io/blog/2023/jmp/"><![CDATA[<div align="center" class="buttonbar">[ <i>JOB MARKET PAPER</i> | <a href="/assets/pdf/papers/Bradt_JMP.pdf" target="_blank">DOWNLOAD PDF</a> ]</div> <p>Are subsidies to consumers effective industrial policy?</p> <p>This question is important for several active policy debates — perhaps none more so than efforts to decarbonize the US economy, where supporting the adoption of early-stage, clean technologies through demand subsidies is increasingly popular.</p> <p>Much of the debate surrounding these interventions highlights the potential for industry growth, a goal more often associated with conventional industrial policies such as entry or production subsidies. Proponents of consumer subsidies on these grounds argue that greater demand increases industry learning-by-doing, which in turn reduces costs for potential entrants.</p> <p>However, economic theory emphasizes that learning spillovers make experience-based cost reductions a public good, which reduces firms’ incentives to expand output and lower costs. The net effect of consumer subsidies on growth in target industries therefore depends on the extent to which the public good nature of learning reduces incentives to lower costs.</p> <p>To answer this question in the real world, we need to know something about firms’ learning curves and how firms make entry, exit, and output decisions.</p> <p><br/></p> <p><span style="font-size:1.5em; color:#0076df">My Focus: California Solar Installers</span><br/> I examine this tradeoff in a policy-relevant setting: consumer subsidies for solar photovoltaics (PV). I study the impact of this form of policy on solar installation firms, which account for a <a href="https://emp.lbl.gov/sites/default/files/5_tracking_the_sun_2023_report.pdf">growing share</a> of the final cost to consumers in this industry.</p> <p>To assess this industry empirically, I focus on the market for residential solar PV in California, which is home to half of all US residential PV systems and has extensive experience subsidizing household solar adoption over the past few decades. One of the largest subsidy programs in the state was the <a href="https://www.cpuc.ca.gov/-/media/cpuc-website/about-cpuc/documents/transparency-and-reporting/fact_sheets/csifactsheet_v4.pdf">California Solar Initiative (CSI)</a>, which ran from 2007 to 2013 and provided $2.2 billion in direct cash rebates to households that adopted solar PV.</p> <p>Understanding the impact of California’s solar PV subsidies on market size and structure requires knowledge of how firms in this industry learn. The arguments in favor of consumer subsidies as a tool to spur industry growth assume that firms become more efficient as they gain experience.</p> <p>Do solar installers experience learning-by-doing? What do installers’ learning curves look like?</p> <p><br/></p> <p><span style="font-size:1.5em; color:#0076df">Solar Installers’ Costs Decline with Experience</span><br/> <span style="font-size:1.25em; color:#828282">Average Costs Dropped 65%, over 2008-2013</span><br/></p> <p>Using data on over 50,000 residential solar systems in California for 2008-2013, I develop and estimate a dynamic structural model of the market for solar installations. The model allows me to not only recover solar installers’ learning curves — including the extent to which knowledge spills over across firms — but also simulate firm behavior under counterfactual policies.</p> <div class="blogIframe"> <iframe src="/assets/plotly/p_mc_est.html" frameborder="0" scrolling="no"></iframe><br/> </div> <p>I find that solar installers’ costs decline dramatically with the quantity of cumulative production — i.e., “experience.” On average, installers’ marginal costs per unit of installed PV capacity dropped $1.70 over 2008-2013 in California, a 68% reduction.</p> <p>These estimates of installers’ costs match both the magnitude and time-path of <a href="https://www.nrel.gov/docs/fy22osti/83586.pdf">publicly available estimates</a> produced by the National Renewable Energy Laboratory (NREL). As I show above, estimating a static model of installers’ decisions gives vastly different estimates from NREL, which emphasizes the importance of a number of key features of my model.</p> <p>So solar installers do experience learning-by-doing. But how much of this is firms learning on their own versus benefitting from the knowledge of others?</p> <p><br/></p> <p><span style="font-size:1.5em; color:#0076df">Observed Cost Reductions Are Mainly Driven by Spillovers</span><br/> <span style="font-size:1.25em; color:#828282">Though the Marginal Effect of Own Experience is 25% Larger </span><br/></p> <p>Though I find that on the margin firms’ own experience contributes more to their learning, I estimate that the vast majority of observed cost reductions come from knowledge spillovers across firms.</p> <div class="blogIframe"> <iframe src="/assets/plotly/p_sc_avg.html" frameborder="0" scrolling="no"></iframe><br/> </div> <p>I test several models of firm spillovers, including constant spillovers, spillovers that differ based on whether rival firms are in the same geographic market (i.e., county), and spillovers that differ based on whether rival firms install the same type of panel. Across all models, cost reductions from rivals’ learning far outweigh those from a firm’s own learning.</p> <p>Solar installers learn with experience and this knowledge spills over across firms. What role do these large knowledge spillovers play in determining the impact of consumer subsidies in this industry?</p> <p><br/></p> <p><span style="font-size:1.5em; color:#0076df">CA Consumer Subsidies Increased Market Size</span><br/> <span style="font-size:1.25em; color:#828282">Increased Number of Firms by 9%, Installations by 4% over 2008-2013</span></p> <p>I use my model to simulate the industry with and without the California Solar Initiative (CSI)’s consumer subsidies. I find that the number of active firms and the number of solar installations decrease by 9% and 4% without the CSI, respectively. Click “Simulate Industry” below to see what this looks like.</p> <div class="blogIframe"> <iframe src="/assets/plotly/p_csi_combined.html" frameborder="0" scrolling="no"></iframe> </div> <p>The fact that the CSI’s subsidies result in both demand- and supply-side growth suggests that any disincentive firms face to take advantage of learning-by-doing is outweighed by its benefits. Despite large knowledge spillovers across firms, industry cost reductions due to the additional experience from consumer subsidies outweigh the firms’ strategic incentives to influence the costs of their rivals by producing less.</p> <p>If policymakers approach to decarbonizing the economy involves targeting growth in specific clean technologies—perhaps as a result of political constraints on first-best policy tools like a carbon tax—consumer subsidies may be an effective means of doing so.</p> <p>So consumer subsidies can be effective industrial policy. But how do they compare to more conventional forms of industrial policy?</p> <p><br/></p> <p><span style="font-size:1.5em; color:#0076df">Entry Subsidies Outperform Consumer Subsidies in CA</span><br/> <span style="font-size:1.25em; color:#828282">Lead to Increase in Active Firms and Installations, Lower Costs</span><br/></p> <p>I use my model to implement a set of counterfactual policies in which I remove the California Solar Initiative (CSI)’s consumer subsidies and replace them with entry subsidies of varying sizes. These entry subsidies are similar in total magnitude to the CSI, ranging $2.1–8.0 billion in cost.</p> <div class="blogIframe"> <iframe src="/assets/plotly/p_entrysubs_combined.html" frameborder="0" scrolling="no"></iframe> </div> <p>The entry subsidies greatly increase not only the number of active incumbents, but also the number of solar installations, resulting in an increase in aggregate welfare.</p> <p><br/></p> <p><span style="font-size:1.5em; color:#0076df">Key Takeaways</span><br/> These findings suggest that consumer subsidies can be an appealing second-best tool for policymakers to achieve decarbonization and industrial policy goals in specific technologies. However, my results indicate that other approaches such as entry subsidies may be more effective on these outcomes.</p> <p>While the empirical results of this paper are technology- and context-specific, they are likely informative for similar policies in other areas. For example, heat pumps are a closely related clean technology with a similar set of intermediary installers.</p> <p>I also compare the CSI’s consumer subsidies to a number of alternative consumer rebate designs and climate policies, including a carbon tax. I leave those results for the paper, which you can <a href="/assets/pdf/papers/Bradt_JMP.pdf" target="_blank">download here</a>.</p> <p><br/></p> <div align="center" class="buttonbar">[ <a href="/assets/pdf/papers/Bradt_JMP.pdf" target="_blank">READ THE PAPER</a> | <a href="/assets/pdf/cv/bradt_cv.pdf" target="_blank">VIEW MY CV</a> ]</div> <p><br/></p> <p><span style="font-size:1.25em; color:#828282">Methodology</span></p> <p class="methodology">I develop a dynamic structural model of the market for residential solar PV installations in California based on the theoretical framework for dynamic oligopoly of <a href="https://www.jstor.org/stable/2297841" target="_blank">Ericson and Pakes (1995)</a>. The model endogenizes consumer demand for solar PV installations as well as installation firms' entry, exit and quantity-setting decisions. Consumer demand for differentiated solar PV installations is static and follows the random coefficient nested logit model of <a href="https://www.jstor.org/stable/40004956" target="_blank">Brenkers and Verboven (2006)</a>. Incumbent installers' installation-specific costs are a function of their own cumulative production as well as the cumulative production of their rivals to allow for learning-by-doing and learning spillovers. In each geographic market, incumbent firms dynamically choose a quantity of installations to provide conditional on their marginal costs and their beliefs about future learning. I model firms' product market decisions as dynamic to capture the incentives to select a production level today based on its impact on own and rival costs in the future. Incumbent firms then choose whether to exit by comparing their expected discounted future profits with an idiosyncratic scrap value while a market-specific pool of potential entrants make one-shot entry decisions based on their expected discounted future profits and an idiosyncratic cost of entry. Firms' strategies lead to a Markov Perfect Equilibrium, which I assume is well-approximated by a Moment-based Markov Equilibrium concept <a href="https://www.jstor.org/stable/45106774" target="_blank">Ifrach and Weintraub (2017)</a>.</p> <p class="methodology">I estimate the model using detailed, system-level data on prices, rebates, installed capacities, and hardware costs for a substantial share of California residential PV systems from 2008 to 2013. I acquire the bulk of the system-level data from the Lawrence Berkeley National Lab's "Tracking the Sun" database, which includes detailed data on the near universe of PV systems installed from 2000 to 2021 in the US <a href="https://emp.lbl.gov/tracking-the-sun" target="_blank">Barbose et al. (2022)</a>. The "Tracking the Sun" database includes detailed data on over 2.5 million PV systems installed from 2000 to 2021 which are collected annually from state agencies and utilities that administer PV incentive programs, registration systems for renewable energy credits, or grid interconnection processes. I combine these data with information on hardware costs associated with specific residential installations in California, which I acquire from the California Public Utilities Commission. These data, which are broadly available for California PV systems installed between 2008 and 2013, are important as they allow me to isolate the component of installers' marginal costs in which learning-by-doing could occur, which I otherwise do not observe. Given the importance of these hardware cost data to isolating installer learning, I subset the combined PV system-level data to those California systems installed between 2008 and 2013 before aggregating the data to the market-time-level for all observed installers.</p> <p class="methodology">My approach to estimation builds on the family of two-step estimators of dynamic games and their various applications. The main primitives of the model include: the demand system for residential solar PV installations, firms' marginal installation cost function, and the distributions of scrap values and entry costs. In the first stage, I recover the static demand parameters and flexibly estimate the exit policy function and transition process of state variables from the data. Firms' value functions, which approximate expected discounted future profits, are nonlinear in the target production cost parameters based on necessary functional form assumptions. I therefore use the first stage estimates to obtain a flexible approximation of the value function following recent work in the dynamic games literature. In the second stage, I form moments from the model's optimal quantity-setting and exit conditions to first recover the parameters governing production costs (i.e., learning) and exit. I then use these estimates to formulate the likelihood of observed entry decisions to recover the full set of dynamic parameters of interest. Having recovered the main parameters of the model, I evaluate the implications of various counterfactual policy environments, simulating California's solar PV installation industry over time and turning on and off different policies.</p> <p class="methodology">I solve the model under counterfactuals using a two-step approach similar to parametric policy iteration. Solving the model involves two steps: first, solving for the new Bellman equation, policy functions, and product market equilibrium in a given period and second simulating the industry forward one period. In each of the counterfactual scenarios that I describe below, I initiate this two-step procedure at the observed data in the first period of my main estimation sample---the first quarter of 2008---and then repeat the two-step procedure until I reach the end of the main estimation sample---the last quarter of 2013. This produces a single industry path over the full counterfactual period. I repeat this process multiple times, averaging market outcomes across a number of distinct, forward-simulated industry paths. In practice, I repeat the process of simulating the model forward over the 6 years in the estimation sample 60 distinct times for each counterfactual scenario and then average key outcomes across all 60 model runs.</p>]]></content><author><name>Jacob Bradt</name></author><summary type="html"><![CDATA[Unconventional Industrial Policy in the US Residential Solar Industry]]></summary></entry><entry><title type="html">Speeding up spatial operations in R</title><link href="https://jtbradt.github.io/blog/2022/spatialbatch/" rel="alternate" type="text/html" title="Speeding up spatial operations in R"/><published>2022-07-15T00:00:00+00:00</published><updated>2022-07-15T00:00:00+00:00</updated><id>https://jtbradt.github.io/blog/2022/spatialbatch</id><content type="html" xml:base="https://jtbradt.github.io/blog/2022/spatialbatch/"><![CDATA[<p>I am a <em>big</em> fan of R’s spatial libraries, especially <a href="https://cran.r-project.org/web/packages/sf/index.html">sf</a> for vector data and <a href="https://cran.r-project.org/web/packages/terra/index.html">terra</a> for raster data. In fact, I could write a great essay about the reasons why you should use R for your spatial analysis (it makes things easily replicable, it integrates well with other languages you might use in a project, it’s free, etc.), but I’ll leave that subtweet of ArcGIS for another day.</p> <p>For all of their magical features, I do sometimes find myself confronting non-trivial memory/timing barriers when using the R spatial libraries locally for my projects. This is particularly true when—as is often the case for me—I need to do things involving <a href="https://r-spatial.github.io/sf/reference/geos_binary_pred.html">geometric binary predicates</a> with many different geometries.</p> <p>One way that I’ve been getting around this recently is through batch processing. Making use of a pretty simple helper function from the <code class="language-plaintext highlighter-rouge">sf</code> package, <code class="language-plaintext highlighter-rouge">sf::st_make_grid()</code>, I can easily break up my spatial operations into more memory-manageable chunks.</p> <p>Say, for instance, I have a bunch of points for each of which I want to identify points falling a certain distance, say a mile. The sf package has a nice function for this: <code class="language-plaintext highlighter-rouge">sf::st_is_within_distance()</code>; however, if I have a lot of points using this function in the “standard” way would involve a non-trivial number of distance calculations (e.g., 10,000 points would mean 100,000,000 distance calculations!!). And most of those calculations are useless: I can easily rule out certain point-pairs as being outside of the target distance of say 1 mile.</p> <p>Let’s make this example a bit more concrete. Say we have 10,000 points within the state of California chosen at random. I can use the function <code class="language-plaintext highlighter-rouge">sf::st_make_grid()</code> to break up the spatial extent of this area into a specified number of grids (actually hexagons, but you can also do square grids — the hexagons fit spatial data a bit better). Once I’ve done that, I can make use of the <code class="language-plaintext highlighter-rouge">sf::st_join()</code> function to assign each of my points to a given grid, then I can loop over each grid when doing my intensive spatial operations.</p> <figure class="highlight"><pre><code class="language-r" data-lang="r"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
</pre></td><td class="code"><pre><span class="c1"># Load dependencies:</span><span class="w">
</span><span class="n">pacman</span><span class="o">::</span><span class="n">p_load</span><span class="p">(</span><span class="n">sf</span><span class="p">,</span><span class="w"> </span><span class="n">tigris</span><span class="p">)</span><span class="w">

</span><span class="c1"># Download California county shapefiles:</span><span class="w">
</span><span class="n">ca.counties</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">tigris</span><span class="o">::</span><span class="n">counties</span><span class="p">(</span><span class="n">state</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"CA"</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
	</span><span class="n">st_transform</span><span class="p">(</span><span class="n">.</span><span class="p">,</span><span class="w"> </span><span class="n">crs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">st_crs</span><span class="p">(</span><span class="m">3310</span><span class="p">))</span><span class="w">
  
</span><span class="c1"># Sample points:</span><span class="w">
</span><span class="n">sample.points</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">st_sample</span><span class="p">(</span><span class="n">ca.counties</span><span class="p">,</span><span class="w"> </span><span class="m">10000</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
	</span><span class="n">st_sf</span><span class="w">
  
</span><span class="c1"># Create 10x10 grid:</span><span class="w">
</span><span class="n">ca.grid</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">st_make_grid</span><span class="p">(</span><span class="n">ca.state</span><span class="p">,</span><span class="w"> </span><span class="n">n</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">10</span><span class="p">,</span><span class="m">10</span><span class="p">),</span><span class="w"> </span><span class="n">square</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">F</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
	</span><span class="n">st_sf</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>The <code class="language-plaintext highlighter-rouge">square=F</code> argument makes a grid of hexagons with the same extent of the California county polygons. Now if you’re paying close attention, you might worry about how points at the edges of each grid are handled with this approach. Surely there might be some points falling within the target distance for points on the boundary of a given grid that are in other grids. To get around this, we can actually buffer the grid polygons we’ve created by the target distance—here I’m using a mile—and <em>then</em> do the <code class="language-plaintext highlighter-rouge">st_join()</code> with our points:</p> <figure class="highlight"><pre><code class="language-r" data-lang="r"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="code"><pre><span class="c1"># Create one-mile buffer around CA grid:</span><span class="w">
</span><span class="n">ca.grid.buffer</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">st_buffer</span><span class="p">(</span><span class="n">ca.grid</span><span class="p">,</span><span class="w"> </span><span class="n">dist.within</span><span class="p">)</span><span class="w">

</span><span class="c1"># Intersect buffered grid with sampled points:</span><span class="w">
</span><span class="n">sample.points</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">st_join</span><span class="p">(</span><span class="n">sample.points</span><span class="p">,</span><span class="w"> </span><span class="n">ca.grid.buffer</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>The side-by-side images below show the initial problem and the resulting batch assignments.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/fig_map_ca_nogrid-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/fig_map_ca_nogrid-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/fig_map_ca_nogrid-1400.webp"/> <img src="/assets/img/fig_map_ca_nogrid.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> &lt;/div&gt; <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/fig_map_ca_grid-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/fig_map_ca_grid-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/fig_map_ca_grid-1400.webp"/> <img src="/assets/img/fig_map_ca_grid.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> &lt;/div&gt; &lt;/div&gt; <div class="caption"> Assigning batches by spatial proximity to a random set of California points. </div> Now if I want to identify points falling within a mile of each point, I could just do a standard application of the `sf` package's `st_is_within_distance` function <figure class="highlight"><pre><code class="language-r" data-lang="r"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="code"><pre><span class="c1"># Standard approach:</span><span class="w">
</span><span class="n">standard</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">microbenchmark</span><span class="p">(</span><span class="w">
	</span><span class="n">st_is_within_distance</span><span class="p">(</span><span class="n">sample.points</span><span class="p">,</span><span class="w"> </span><span class="n">dist</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">dist.within</span><span class="p">,</span><span class="w"> </span><span class="n">remove_self</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">T</span><span class="p">)</span><span class="w">
</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure> Again, if I have a lot of points (like 10,000 or more) this will quickly become difficult for my computer to handle. So I can instead batch process using data.table's magic: <figure class="highlight"><pre><code class="language-r" data-lang="r"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="code"><pre><span class="c1"># Batch process:</span><span class="w">
</span><span class="n">batch</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">microbenchmark</span><span class="p">(</span><span class="w">
	</span><span class="n">sample.points</span><span class="p">[</span><span class="w"> </span><span class="p">,</span><span class="w"> </span><span class="n">.</span><span class="p">(</span><span class="n">res</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">list</span><span class="p">(</span><span class="n">st_is_within_distance</span><span class="p">(</span><span class="n">.SD</span><span class="p">[[</span><span class="s1">'geometry'</span><span class="p">]],</span><span class="w"> </span><span class="n">dist</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">dist.within</span><span class="p">,</span><span class="w"> </span><span class="n">remove_self</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">T</span><span class="p">))),</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">id</span><span class="p">]</span><span class="w">
</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure> What are the time savings? I did each of the above approaches for different numbers of points and it really does not take long for the batched approach to beat out the standard approach. <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/fig_spatial_batch-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/fig_spatial_batch-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/fig_spatial_batch-1400.webp"/> <img src="/assets/img/fig_spatial_batch.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> &lt;/div&gt; &lt;/div&gt; For even larger problems, this opens up the possibility of parallel processing. The `sf::st_is_within_distance()` function is also not the most efficient of the sf packages offerings, so even greater speedups are likely possible for this particular use case. </figure></div></div></figure></div></figure></div></div>]]></content><author><name></name></author><summary type="html"><![CDATA[Combining `sf` and `data.table` libraries to batch process spatial data in R]]></summary></entry><entry><title type="html">Calculating driving distances using Open Source Routing Machine</title><link href="https://jtbradt.github.io/blog/2022/osrm-aws/" rel="alternate" type="text/html" title="Calculating driving distances using Open Source Routing Machine"/><published>2022-06-01T00:00:00+00:00</published><updated>2022-06-01T00:00:00+00:00</updated><id>https://jtbradt.github.io/blog/2022/osrm-aws</id><content type="html" xml:base="https://jtbradt.github.io/blog/2022/osrm-aws/"><![CDATA[<p>A while back I found myself needing to calculate driving distances and times between 30,744 origins and 408 destinations in the continental US. That’s a total of more than 12 million routes on a huge network! Ordinarily something like <a href="https://developers.google.com/maps/documentation/directions/overview">Google’s Directions API</a> would be helpful, but with the volume of routes and my grad student research budget this was not an option.</p> <p>Thankfully there’s a great product out there called <a href="http://project-osrm.org/">Open Source Routing Machine (OSRM)</a> that I was able to use for the task. Given the size of the road network I needed for my task, I did need to make use of virtual computing to run OSRM; however, I was able to do so at a tiny fraction of the cost of Google’s Directions API. Open-source for the win!</p> <p>Getting started took quite a bit of learning-by-doing. To provide positive learning-by-doing spillovers, I thought I would provide a brief<d-footnote>On second thought, maybe not so brief.</d-footnote> guide to setting up and running an OSRM server on an <a href="https://aws.amazon.com/">Amazon Web Services (AWS)</a> remote computing instance (and querying the server in R). OSRM is a valuable tool for calculating driving distances and times for a large number of origins and destinations at relatively low cost, particularly compared to other paid services such as the Google Map Direction API. OSRM is designed to be run using open source road network data from the <a href="https://www.openstreetmap.org/#map=4/38.01/-95.84">Open Street Map (OSM)</a> project; setting aside the fixed costs associated with learning how to use it, OSRM therefore provides a powerful routing option at no cost.</p> <p>Most applied researchers are likely interested in calculating driving distances and times for relatively large geographies. OSRM enables shortest path computation on continental-sized road networks; however, to do so requires sufficient memory and disk size and can take time to setup. If you are interested in, for example, calculating drive distances and times for the continental U.S., it is unlikely that you will have the technical requirements to do so locally. That’s why running OSRM on an AWS EC2 instance is a natural solution that adds minimal cost; fixed costs of learning how to use AWS aside, this is still far less expensive when running a large number of routes than paid services.</p> <p>I outline the complete process that I took to calculate drive distances and times for a complete matrix of origins and destinations in the continental U.S. via and AWS EC2 instance. Some or all of the below steps may or may not apply to you—free disposal of course applies so please use whatever is helpful in your application. Even if you, say, choose to use an alternative remote computing approach, the same general formula should still apply:</p> <ol> <li>Set up your remote instance (via AWS, etc.)</li> <li>Set up the OSRM server for your desired road network</li> <li>Run your queries!</li> </ol> <p>A few additional disclaimers. I am using a Mac running Mojave; if you are using a different operating system, the processes for accessing your AWS instance are likely to be different (thankfully AWS has ample documentation). I also made specific decisions on how to go about setting up the OSRM server on my instance—e.g., running a Linux OS on the instance, building OSRM using a Docker image, etc.—which are by no means the only way to go about doing this. I outline my rationale below, but alternative approaches may be better for your own purposes. Hopefully this resources is useful in your projects; if you have any questions, feel free to contact me by <a href="mailto:jbradt@g.harvard.edu">email</a>.</p> <h2 id="setting-up-your-aws-instance">Setting up your AWS instance</h2> <p>For my project calculating continental U.S. drive distances and durations, I ran the OSRM server on the entire U.S. road network (a very large network; the *.pbf file—more on this below—is 7.2 GB). To make sure that I had more than enough memory, I set up an m4.10xlarge instance. Generally, I think that an M4 instance is a good choice for this kind of task; AWS says that they provide a balance of compute, memory, and network resources. Your choice of a specific instance should depend on your memory/virtual CPU requirements. The table below shows the different M4 configurations (the full set of instance types is available <a href="https://aws.amazon.com/ec2/instance-types/">here</a>).</p> <table> <thead> <tr> <th>Instance</th> <th style="text-align: center">vCPU</th> <th style="text-align: center">Memory (GB)</th> </tr> </thead> <tbody> <tr> <td>m4.large</td> <td style="text-align: center">2</td> <td style="text-align: center">8</td> </tr> <tr> <td>m4.xlarge</td> <td style="text-align: center">4</td> <td style="text-align: center">16</td> </tr> <tr> <td>m4.2xlarge</td> <td style="text-align: center">8</td> <td style="text-align: center">32</td> </tr> <tr> <td>m4.4xlarge</td> <td style="text-align: center">16</td> <td style="text-align: center">64</td> </tr> <tr> <td>m4.10xlarge</td> <td style="text-align: center">40</td> <td style="text-align: center">160</td> </tr> <tr> <td>m4.16xlarge</td> <td style="text-align: center">64</td> <td style="text-align: center">256</td> </tr> </tbody> </table> <p>AWS offers two pricing schemes which depend on how you choose to schedule your EC2 usage: on-demand instances which allow you to use your instance whenever you like versus spot instances which allow you to make requests for spare EC2 computing capacity. You end up getting up to 90% off if you go the spot instance route. Additional info on pricing is available <a href="https://aws.amazon.com/ec2/pricing/">here</a>.</p> <p>Once you’ve decided which instance type and a pricing option is best for your project, the next key decision is which operating system you want to run on your instance. This will have important implications for implementing the steps that follow. I had my instance run the Amazon Linux AMI; this is the default and running Linux was helpful given that a lot of the OSRM backend support is geared towards Linux or Ubuntu OS.</p> <p>If you’re following the steps in this guide, I would suggest the default Amazon Linux AMI for your instance so that the specific steps included below are relevant to you. Once you’ve chosen an instance type, pricing option, and machine image, you are ready to get your instance up and running. But first you need to set up your key pair. This is the security credentials that you will use to prove your identity/access permissions when connecting to your instance. You can do so by following Amazon’s tutorial for EC2 key pairs and Linux instances: <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html#having-ec2-create-your-key-pair">Amazon EC2 key pairs and Linux instances</a>. I would recommend naming your key pair for this project something straightforward, like “osrm-instance” and make sure that you save it to a local directory you can easily navigate to via the command line as a *.pem file.<d-footnote>If you are using Windows OS locally and want to use a third party SSH client like PuTTY, you'll need to save your key as a *.ppk file. I think that new Windows OS already have an SSH client, OpenSSH. Since I run a Mac OS, I am not familiar with this client, but AWS has a helpful tutorial on connecting to Amazon Linux instances using any SSH client: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AccessingInstancesLinux.html. If you use OpenSSH on Windows, you should be able to use your key pair in the *.pem file format.</d-footnote> Note that you can only download a file storing your key pair once, so make sure you know which file format you need (see Footnote 3).</p> <p>Rather than copying directly from the AWS guides, here’s the link to a tutorial that walks you through the launch process for the Amazon Linux AMI: <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EC2_GetStarted.html#ec2-launch-instance">Amazon Linux instance setup tutorial</a>.</p> <p>In step 4 of setting up your instance via the EC2 console, you’ll be asked to customize the elastic block storage with which your instance is launched. I added 128 GB of EBS, but I also ran a few projects using the OSRM server entirely remotely on my instance. This is not necessary if you follow the steps outlined below to run queries locally using the OSRM server you set up on your AWS instance. You will also be asked in step 6 to setup security groups. I would suggest setting up two TCP protocols, both specified to accept traffic from your IP address as the sole source: one standard port 22 protocol and the other a custom TCP protocol for port 5000; it will be important that port 5000 is open to your IP when it comes time to send queries to your OSRM server.<d-footnote>You can always add protocols after you launch your instance in the EC2 console.</d-footnote></p> <h2 id="setting-up-osrm">Setting up OSRM</h2> <p>Once your Amazon Linux instance is up and running, it is time to connect to it and begin setting up OSRM. I would encourage you to take a look at the <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AccessingInstancesLinux.html">AWS tutorial</a> on connecting to your instance using an SSH client. This can be done from the command line when using Mac OS:</p> <figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="nb">cd</span> ~/directory_containing_pem
ssh <span class="nt">-i</span> <span class="s2">"osrm-instance.pem"</span> ec2-user@[Your Public DNS]</code></pre></figure> <p>The Public DNS for your instance is available in the EC2 console. An example (expired) Public DNS is “ec2-18-221-51-143.us-east-2.compute.amazonaws.com”.</p> <p>I think that the easiest way to build OSRM on your instance is to use Docker. The team behind OSRM publish ready to use lightweight Docker images for each OSRM release. This approach has several benefits. First, you don’t need to worry about cloning OSRM backend, which would require you to have a Github account and be prompted to enter your username and password. Using the Docker images also means that you don’t have to install the large number of tools and libraries required to build and run osrm-backend “native” on your EC2 instance. Additional info on the Docker images published by Project OSRM is available on the <a href="https://github.com/Project-OSRM/osrm-backend">Project-OSRM/osrm-backend Github page</a> and on the <a href="https://hub.docker.com/r/osrm/osrm-backend/">Project OSRM dockerhub page</a>. The OSRM backend Docker images are quite lightweight, so there’s little in the way of a tradeoff in terms of overhead for the ease of use that you get.</p> <p>To build osrm-backend using the latest published Docker image, first, set up Docker on your EC2 instance (<a href="https://docs.docker.com/">docker docs</a>). After connecting to your instance, you can execute the following steps via the command line to set up Docker:</p> <figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="nb">sudo </span>yum update <span class="nt">-y</span>
<span class="nb">sudo </span>yum <span class="nb">install </span>docker <span class="nt">-y</span>
<span class="nb">sudo </span>service docker start
<span class="nb">sudo </span>usermod <span class="nt">-a</span> <span class="nt">-G</span> docker ec2-user</code></pre></figure> <p>Docker should now be set up on your instance. You then need to log out and log back in so that your group membership is re-evaluated:</p> <figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="nb">exit
</span>ssh <span class="nt">-i</span> <span class="s2">"osrm-instance.pem"</span> ec2-user@[Your Public DNS]
docker info</code></pre></figure> <p>The final line checks that Docker is up and running. Now that you’re all set with Docker, the next step is to download/upload OpenStreetMap extracts to your instance. There are several ways of doing this, but by far the easiest is to use <a href="http://download.geofabrik.de/">Geofabrik</a>. Geofabrik offers easy downloads of OSM road networks for pretty much any aggregate geography you could want. In our case, it allows us to bypass downloading OSM networks locally and then uploading them to our instance: you can just fetch the data directly in your instance using the following command-line code:</p> <figure class="highlight"><pre><code class="language-shell" data-lang="shell">wget https://download.geofabrik.de/north-america/us-latest.osm.pbf</code></pre></figure> <p>Where this is fetching the latest OSM network for the entire US (about 7.2 GB). To find the download address for your desired geography, simply look for it on the <a href="http://download.geofabrik.de/">Geofabrik</a> site.</p> <p>Once you’ve fetched the relevant road network, it is time to pre-process the network extract. I assume that you are looking for driving directions and times, so we will do so with the car profile. To do so, execute the following commands separately:</p> <figure class="highlight"><pre><code class="language-shell" data-lang="shell">docker run <span class="nt">-t</span> <span class="nt">-v</span> <span class="si">$(</span><span class="nb">pwd</span><span class="si">)</span>:/data osrm/osrm-backend osrm-extract <span class="nt">-p</span> /opt/car.lua /data/us-latest.osm.pbf
docker run <span class="nt">-t</span> <span class="nt">-v</span> <span class="si">$(</span><span class="nb">pwd</span><span class="si">)</span>:/data osrm/osrm-backend osrm-partition /data/us-latest.osrm
docker run <span class="nt">-t</span> <span class="nt">-v</span> <span class="si">$(</span><span class="nb">pwd</span><span class="si">)</span>:/data osrm/osrm-backend osrm-customize /data/us-latest.osrm</code></pre></figure> <p>The first line creates an OSRM object from the downloaded road network using the car profile (can also switch to bicycle and walking profiles, but if you’re doing that you probably aren’t using large networks, can run OSRM locally, and don’t need this guide!). The second and third lines create the contraction hierarchies, which facilitate the shortest route between two points. Note that you will have to change “us-latest.osm.pbf” and “us-latest.osrm” in these commands to match the road network you are using. Each of these three commands will take a while to run, and total run time depends on the size of the road network you are using. Running this on the US road network took about 2-3 hours total.</p> <p>You are now almost ready to send routing queries to your OSRM server. The final step is to start a routing engine HTTP server on port 5000:</p> <figure class="highlight"><pre><code class="language-shell" data-lang="shell">docker run <span class="nt">-t</span> <span class="nt">-i</span> <span class="nt">-p</span> 5000:5000 <span class="nt">-v</span> <span class="si">$(</span><span class="nb">pwd</span><span class="si">)</span>:/data osrm/osrm-backend osrm-routed <span class="nt">--algorithm</span> mld /data/us-latest.osrm</code></pre></figure> <p>This final step should take a few moments, but at the end you will get the following message: “running and waiting for requests” — you’ve done it! Your OSRM server is up and running and ready to receive queries.</p> <p>Now that you’ve gone through the process of setting up your instance and getting your OSRM server up and running, you can exit your connection to your instance at any time and return to run additional queries when you want. To do so, simply re-connect to your instance and run the final command above that you used to start your routing engine HTTP server on port 5000.<d-footnote>NOTE: you will want to be considerate about the time that you keep your instance up and running (as opposed to paused or even terminated) as you will continue to be charged, certainly for the considerable amount of data that you may be storing on the EBS device that you've attached to your instance. I would encourage you to keep track of usage and charges via your AWS billing dashboard.</d-footnote></p> <h2 id="running-osrm-queries">Running OSRM queries</h2> <p>Now that your OSRM server is up and running, you can test to make sure it is running as expected by navigating to the following URL:</p> <figure class="highlight"><pre><code class="language-html" data-lang="html">http://your_server_ip:5000/route/v1/driving/13.388860,52.517037;13.385983,52.496891?steps=false</code></pre></figure> <p>where “your_server_ip” will be the Public DNS for your instance. You should get a JSON result if everything is running as expected.</p> <h2 id="example-r-code">Example R code</h2> <p>To get driving distances and durations for a large set of origin-destination pairs, I used the <a href="https://cran.r-project.org/web/packages/osrm/index.html">osrm R package</a> with the OSRM server option set to the IP address of the OSRM server I set up. Specifically, since I needed driving distances and durations from each origin to every destination for 30,744 origins and 408 destinations, I used the “osrmTable” function from the osrm R package, sending calls to the server in batches of 10 origins at a time. I’ve included the R function that I used to implement this as perhaps a helpful starting point for your own project.</p> <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#	Load packages:</span><span class="w">
</span><span class="n">pacman</span><span class="o">::</span><span class="n">p_load</span><span class="p">(</span><span class="n">data.table</span><span class="p">,</span><span class="w"> </span><span class="n">dplyr</span><span class="p">,</span><span class="w"> </span><span class="n">tidyr</span><span class="p">,</span><span class="w"> </span><span class="n">osrm</span><span class="p">,</span><span class="w"> </span><span class="n">parallel</span><span class="p">,</span><span class="w"> </span><span class="n">rlist</span><span class="p">)</span><span class="w">

</span><span class="c1">#	Function to batch generate large driving distance and duration matrices using specified OSRM server:</span><span class="w">
</span><span class="n">drive.distance</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">origin.file</span><span class="p">,</span><span class="w"> </span><span class="n">dest.file</span><span class="p">,</span><span class="w"> </span><span class="n">chunk.size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">10</span><span class="p">,</span><span class="w"> </span><span class="n">server</span><span class="p">,</span><span class="w"> </span><span class="n">num.cores</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="n">output.dir</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">NULL</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
	
	</span><span class="c1">#	Load origins data:</span><span class="w">
	</span><span class="n">origin.data</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">fread</span><span class="p">(</span><span class="n">origin.file</span><span class="p">)</span><span class="w">
	
	</span><span class="c1">#	Split origins data into chunks:</span><span class="w">
	</span><span class="n">nr.origin</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">nrow</span><span class="p">(</span><span class="n">origin.data</span><span class="p">)</span><span class="w">
	</span><span class="n">origin.chunks</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">split</span><span class="p">(</span><span class="n">origin.data</span><span class="p">,</span><span class="w"> </span><span class="nf">rep</span><span class="p">(</span><span class="m">1</span><span class="o">:</span><span class="nf">ceiling</span><span class="p">(</span><span class="n">nr.origin</span><span class="o">/</span><span class="n">chunk.size</span><span class="p">),</span><span class="w"> </span><span class="n">each</span><span class="o">=</span><span class="n">chunk.size</span><span class="p">,</span><span class="w"> </span><span class="n">length.out</span><span class="o">=</span><span class="n">nr.origin</span><span class="p">))</span><span class="w">
	
	</span><span class="c1">#	Import destination data:</span><span class="w">
	</span><span class="n">dest.data</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">fread</span><span class="p">(</span><span class="n">dest.file</span><span class="p">)</span><span class="w">
	
	</span><span class="c1">#	Set OSRM related options:</span><span class="w">
	</span><span class="n">options</span><span class="p">(</span><span class="n">osrm.server</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">server</span><span class="p">,</span><span class="w"> </span><span class="n">osrm.profile</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"driving"</span><span class="p">)</span><span class="w">
	
	</span><span class="c1">#	Generate duration and distance matrices for each origin and destination pair:</span><span class="w">
	</span><span class="n">matrix</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">mclapply</span><span class="p">(</span><span class="m">1</span><span class="o">:</span><span class="nf">length</span><span class="p">(</span><span class="n">origin.chunks</span><span class="p">),</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">x</span><span class="p">){</span><span class="w">
		</span><span class="n">osrmTable</span><span class="p">(</span><span class="n">src</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">origin.chunks</span><span class="p">[[</span><span class="n">x</span><span class="p">]],</span><span class="w"> </span><span class="n">dst</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">dest.data</span><span class="p">,</span><span class="w"> </span><span class="n">measure</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s1">'duration'</span><span class="p">,</span><span class="w"> </span><span class="s1">'distance'</span><span class="p">))</span><span class="w">
	</span><span class="p">},</span><span class="w"> </span><span class="n">mc.cores</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">num.cores</span><span class="p">)</span><span class="w">
	
	</span><span class="c1">#	Construct duration matrix:</span><span class="w">
	</span><span class="n">durations</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">lapply</span><span class="p">(</span><span class="n">matrix</span><span class="p">,</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">x</span><span class="p">){</span><span class="n">x</span><span class="o">$</span><span class="n">durations</span><span class="p">})</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
		</span><span class="n">do.call</span><span class="p">(</span><span class="n">rbind</span><span class="p">,</span><span class="n">.</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
		</span><span class="n">data.table</span><span class="p">(</span><span class="n">.</span><span class="p">)</span><span class="w">
	</span><span class="n">durations</span><span class="p">[,</span><span class="n">origin.id</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="n">origin.data</span><span class="o">$</span><span class="n">origin.id</span><span class="p">]</span><span class="w">
	
	</span><span class="c1">#	Construct distance matrix:</span><span class="w">
	</span><span class="n">distances</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">lapply</span><span class="p">(</span><span class="n">matrix</span><span class="p">,</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">x</span><span class="p">){</span><span class="n">x</span><span class="o">$</span><span class="n">distances</span><span class="p">})</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
		</span><span class="n">do.call</span><span class="p">(</span><span class="n">rbind</span><span class="p">,</span><span class="n">.</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
		</span><span class="n">data.table</span><span class="p">(</span><span class="n">.</span><span class="p">)</span><span class="w">
	</span><span class="n">distances</span><span class="p">[,</span><span class="n">origin.id</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="n">origin.data</span><span class="o">$</span><span class="n">origin.id</span><span class="p">]</span><span class="w">
	
	</span><span class="c1">#	If requested, write files to given directory:</span><span class="w">
	</span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="o">!</span><span class="nf">is.null</span><span class="p">(</span><span class="n">output.dir</span><span class="p">))</span><span class="w"> </span><span class="p">{</span><span class="w">
		</span><span class="n">output1.filename</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">paste0</span><span class="p">(</span><span class="n">output.dir</span><span class="p">,</span><span class="w"> </span><span class="s2">"driving_durations.csv"</span><span class="p">)</span><span class="w">
		</span><span class="n">output2.filename</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">paste0</span><span class="p">(</span><span class="n">output.dir</span><span class="p">,</span><span class="w"> </span><span class="s2">"driving_distances.csv"</span><span class="p">)</span><span class="w">
		</span><span class="n">fwrite</span><span class="p">(</span><span class="n">durations</span><span class="p">,</span><span class="w"> </span><span class="n">output1.filename</span><span class="p">)</span><span class="w">
		</span><span class="n">fwrite</span><span class="p">(</span><span class="n">distances</span><span class="p">,</span><span class="w"> </span><span class="n">output2.filename</span><span class="p">)</span><span class="w">
	</span><span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="p">{</span><span class="w">
		</span><span class="nf">return</span><span class="p">(</span><span class="nf">list</span><span class="p">(</span><span class="n">distances</span><span class="p">,</span><span class="w"> </span><span class="n">durations</span><span class="p">))</span><span class="w">
	</span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div> <h2 id="final-product">Final product</h2> <p>If you made it this far, the least I can do is show you a pretty(ish) picture. Below is the final product. Note that the above steps actually only produced half of the below image—I also needed to calculate expected flight costs for each of my origin-destination pairs which could be the topic of several other blog posts. But anyways, everyone loves a picture!</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/fig_costs-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/fig_costs-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/fig_costs-1400.webp"/> <img src="/assets/img/fig_costs.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> &lt;/div&gt; &lt;/div&gt; <div class="caption"> Relationship between estimated driving, flying and expected travel costs and one-way driving distance for my origin-destination pairs. Flight probabilities come from https://doi.org/10.1016/j.jeem.2018.06.010 </div> </figure></div></div>]]></content><author><name>Jake Bradt</name></author><summary type="html"><![CDATA[A guide to running Open Source Routing Machine (OSRM) on AWS]]></summary></entry><entry><title type="html">Leveraging behavioral science to close the flood insurance gap</title><link href="https://jtbradt.github.io/blog/2020/wharton/" rel="alternate" type="text/html" title="Leveraging behavioral science to close the flood insurance gap"/><published>2020-03-30T00:00:00+00:00</published><updated>2020-03-30T00:00:00+00:00</updated><id>https://jtbradt.github.io/blog/2020/wharton</id><content type="html" xml:base="https://jtbradt.github.io/blog/2020/wharton/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[Wharton ESG Initiative blog post]]></summary></entry><entry><title type="html">Flood insurance in the US: Lessons from FEMA’s recent data release (part 2)</title><link href="https://jtbradt.github.io/blog/2020/wharton/" rel="alternate" type="text/html" title="Flood insurance in the US: Lessons from FEMA’s recent data release (part 2)"/><published>2020-01-22T00:00:00+00:00</published><updated>2020-01-22T00:00:00+00:00</updated><id>https://jtbradt.github.io/blog/2020/wharton</id><content type="html" xml:base="https://jtbradt.github.io/blog/2020/wharton/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[Wharton ESG Initiative blog post]]></summary></entry><entry><title type="html">Flood insurance in the US: Lessons from FEMA’s recent data release (part I)</title><link href="https://jtbradt.github.io/blog/2020/wharton/" rel="alternate" type="text/html" title="Flood insurance in the US: Lessons from FEMA’s recent data release (part I)"/><published>2020-01-14T00:00:00+00:00</published><updated>2020-01-14T00:00:00+00:00</updated><id>https://jtbradt.github.io/blog/2020/wharton</id><content type="html" xml:base="https://jtbradt.github.io/blog/2020/wharton/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[Wharton ESG Initiative blog post]]></summary></entry></feed>